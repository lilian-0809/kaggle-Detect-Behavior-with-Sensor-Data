1
本大会は、スマートウォッチに内蔵された複数種類のセンサーデータを活用し、利用者の実際のジェスチャー動作を高精度に識別する高効率な予測モデルの開発に挑戦することを目的としている。
大会運営委員会は、マルチクラスのジェスチャー分類におけるモデルの総合的な性能を評価するため、Macro-F1 スコアを主要な評価指標として採用し、評価結果が精度と汎化性能の双方を十分に反映することを保証している。

2
本大会で提供されるデータセットは、81名の参加者から収集された情報で構成されており、各参加者について年齢、性別、身長、腕長などを含む7項目の基本属性が付与されている。
各被験者は約100件の有効なジェスチャーサンプルを提供しており、総学習サンプル数は8151件、対象となるジェスチャーは18種類に及ぶ。センサーデータは多様であり、主に IMU センサー（IMU sensors）、
熱電堆センサー（thermopile sensors）、および飛行時間センサー（Time-of-Flight sensors）の三種類から構成され、これらがマルチモーダルな入力特徴を形成している。

3
本大会で提供される時系列型センサーデータに対しては、まずゲート付き回帰ユニット（GRU）モデルを中核的な処理手法として採用する予定である。GRU は長期依存関係および動的な時系列特徴を効果的に捉える能力を有しており、
これまで多くの時系列分類タスクにおいて優れた性能を示してきたことから、本課題のデータ特性と高い親和性を持つと考えられる。
さらに、飛行時間センサー（ToF）から得られるデータは画像構造に近い形式を有するため、畳み込みニューラルネットワーク（CNN）による分岐構造を導入し、特徴抽出および統合を行う。加えて、モデルの汎化性能を向上させ、
過学習のリスクを抑制する目的で、K 分割交差検証（K-fold Cross-Validation）を採用し、訓練データを5分割して十分な学習および検証を行うことで、モデルの堅牢性と信頼性を高める。
